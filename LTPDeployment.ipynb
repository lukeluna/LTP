{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeeb2ad-3443-408a-9948-6b40fdef61dd",
   "metadata": {},
   "source": [
    "## LTP Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5200e1-fb0a-4bcd-b8eb-64a8206f8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add85130-8b6a-49f8-bfa0-94182bde3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = 'data/handmade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64098530-590d-4177-8d90-c0aa04741fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_hominem = pd.read_csv('{}/ad_hominem_final.csv'.format(DATA_FILEPATH))\n",
    "ad_populum = pd.read_csv('{}/ad_populum_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_anger = pd.read_csv('{}/appeal_to_anger_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_authority = pd.read_csv('{}/appeal_to_authority_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_fear = pd.read_csv('{}/appeal_to_fear_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_nature = pd.read_csv('{}/appeal_to_nature_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_pity = pd.read_csv('{}/appeal_to_pity_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_ridicule = pd.read_csv('{}/appeal_to_ridicule_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_tradition = pd.read_csv('{}/appeal_to_tradition_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_worse_problems = pd.read_csv('{}/appeal_to_worse_problems_final.csv'.format(DATA_FILEPATH))\n",
    "causal_oversimplifiation = pd.read_csv('{}/causal_oversimplification_final.csv'.format(DATA_FILEPATH))\n",
    "equivocation = pd.read_csv('{}/equivocation_final.csv'.format(DATA_FILEPATH))\n",
    "fallacy_of_division = pd.read_csv('{}/fallacy_of_division_final.csv'.format(DATA_FILEPATH))\n",
    "false_analogy = pd.read_csv('{}/false_analogy_final.csv'.format(DATA_FILEPATH))\n",
    "false_causality = pd.read_csv('{}/false_causality_final.csv'.format(DATA_FILEPATH))\n",
    "false_dilemma = pd.read_csv('{}/false_dilemma_final.csv'.format(DATA_FILEPATH))\n",
    "hasty_generalization = pd.read_csv('{}/hasty_generalization_final.csv'.format(DATA_FILEPATH))\n",
    "nothing = pd.read_csv('{}/nothing_final.csv'.format(DATA_FILEPATH))\n",
    "slippery_slope = pd.read_csv('{}/slippery_slope_final.csv'.format(DATA_FILEPATH))\n",
    "strawman = pd.read_csv('{}/strawman_final.csv'.format(DATA_FILEPATH))\n",
    "circular_reasoning = pd.read_csv('{}/circular_reasoning.csv'.format(DATA_FILEPATH))\n",
    "tu_quoque = pd.read_csv('{}/tu_quoque.csv'.format(DATA_FILEPATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1b3dbf-68db-4416-8859-ed139fb29ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_tuples = []\n",
    "nr_of_samples = 150\n",
    "sample = False\n",
    "\n",
    "datasets = [ad_hominem, ad_populum, appeal_to_anger, appeal_to_authority, appeal_to_fear, appeal_to_nature,\n",
    "            appeal_to_pity, appeal_to_ridicule, appeal_to_tradition, appeal_to_worse_problems, causal_oversimplifiation,\n",
    "            circular_reasoning, tu_quoque, #guilt_by_association,\n",
    "            equivocation, fallacy_of_division, false_analogy, false_causality, false_dilemma,\n",
    "            hasty_generalization, nothing, slippery_slope, strawman]\n",
    "\n",
    "for dataset in datasets:\n",
    "    if sample:\n",
    "        if len(dataset) >= nr_of_samples:\n",
    "            # Sample 15 entries if the dataset is large enough\n",
    "            sampled = dataset.sample(n=nr_of_samples, random_state=random.randint(1, 100))  # Change seed for true randomness\n",
    "        else:\n",
    "            # If the dataset has less than 15 entries, take all available data\n",
    "            sampled = dataset\n",
    "    else:\n",
    "        sampled = dataset\n",
    "\n",
    "    # Convert the sampled data to tuples and add to the list\n",
    "    list_of_tuples.extend(sampled.itertuples(index=False, name=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32bc8e82-56d2-4f39-b0d0-a67f3dbe63ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "846"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_of_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c7306420-2a13-4225-99e4-04f16b87f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_prompt(model_id, message_content):\n",
    "    response = ollama.chat(model=model_id, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': message_content\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "295ffa2e-7f01-4cb5-9a4d-6978ce402af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_whole_word(large_string, word):\n",
    "    pattern = rf'\\b{re.escape(word)}\\b'\n",
    "    return bool(re.search(pattern, large_string))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a819de-57fb-4408-868f-b857be656b42",
   "metadata": {},
   "source": [
    "### Zero Shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796272ae-b47b-44f1-a8c5-42439740c128",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the base message content\n",
    "message_base_content = '''\n",
    "Definition:\n",
    "An argument consists of an assertion called the conclusion and one or more assertions called premises, where the premises are intended to establish the truth of the conclusion. Premises or conclusions can be implicit in an argument. \n",
    "A fallacious argument is an argument where the premises do not entail the conclusion. \n",
    "\n",
    "Types of fallacy:\n",
    "hasty generalization \n",
    "slippery slope\n",
    "causal oversimplification\n",
    "appeal to ridicule\n",
    "appeal to nature\n",
    "false causality\n",
    "ad populum\n",
    "ad hominem\n",
    "false analogy\n",
    "false dilemma\n",
    "appeal to fear\n",
    "appeal to (false) authority\n",
    "appeal to worse problems\n",
    "circular reasoning\n",
    "guilt by association\n",
    "appeal to anger                 \n",
    "straw man\n",
    "appeal to tradition \n",
    "equivocation \n",
    "fallacy of division \n",
    "tu quoque \n",
    "appeal to positive emotion \n",
    "appeal to pity\n",
    "\n",
    "I will give you different texts. For each text, determine if it is a fallacy or not. If it is not, or cannot be judged based on the text, write 'nothing'. If it is a fallacy, tell me which fallacy type of the above list it is. Do not give any explanation, just write the answer. If you are unsure, write the one answer that seems most likely: \n",
    "{}\n",
    "'''\n",
    "\n",
    "# List of example texts and their expected labels\n",
    "# examples = [\n",
    "#     ('Why is the sky blue?', 'nothing'),\n",
    "#     ('You have no idea how street is, you better stop talking!', 'ad hominem'),\n",
    "#     (\"You're probably a drug addict yourself.\", 'ad hominem'),\n",
    "#     (\"Why do you want a university-doctor's title to be shown on their identification-card? You must be one yourself, you egotistic prick ! Does it make you feel respected that we all know your little title?\", \"ad hominem\"),\n",
    "#     (\"You’re way too nervous about driving at night so of course you don’t want to drive.\", \"ad hominem\"),\n",
    "#     (\"Three million fans can't be wrong!\", 'ad populum'),\n",
    "#     (\"The human soul is immortal, because all learned men agree that anything which does not come out of the potentiality of matter is incorruptible and immortal.\", \"ad populum\"),\n",
    "#     (\"We all believe such preachers as Mr. Raskin. He is so nearly right, his ideals are so very high, that most people assent — while they have no difficulty in evading them and going on their way as if a breath of wind had fanned their faces, and no voice of truth had stirred their spirits.\", \"ad populum\"),\n",
    "#     (\"You should buy this phone; it's the best-selling model worldwide.\",\"ad populum\"),\n",
    "#     (\"Are you tired of being ignored by your government? Is it right that the top 1% have so much when the rest of us have so little? I urge you to vote for me today!\", 'appeal to anger'),\n",
    "#     (\"I get mad when i think about all these poor guys on the street, having no home, no job, no family. All because they started taking drugs. Its so sad, we need heavier penalties.\",\"appeal to anger\"),\n",
    "# ]\n",
    "\n",
    "examples = list_of_tuples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874b77b5-d8f0-4357-a199-cb4b113d448a",
   "metadata": {},
   "source": [
    "### Chain of Thought"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6344b65-0add-481f-b917-591a7cf9eefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '''few_shot_cot_update'''\n",
    "with open('data/{}.txt'.format(filename), 'r') as file:\n",
    "    message_base_content_cot = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a37e472-3ad5-49c2-9126-11f7f251f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model_id, examples, cot=True, selfconsistency=True):\n",
    "    test_results = []\n",
    "    results = pd.DataFrame()\n",
    "    start_time = time.time() \n",
    "    \n",
    "    for text, expected_label in tqdm(examples):\n",
    "        message_content = message_base_content_cot.format(text) if cot else message_base_content.format(text)\n",
    "\n",
    "        if selfconsistency:\n",
    "            loop = 3\n",
    "            labels = [] \n",
    "            for i in range(loop):\n",
    "                subresponse = ollama_prompt(model_id, message_content)\n",
    "                label = subresponse.strip()\n",
    "                labels.append(label.partition('.')[0].lower())\n",
    "\n",
    "            c = Counter(labels)\n",
    "            actual_label, _ = c.most_common()[0]\n",
    "            \n",
    "        else:\n",
    "            response = ollama_prompt(model_id, message_content)\n",
    "            actual_label = response.strip()\n",
    "            actual_label = actual_label.lower()\n",
    "            \n",
    "        test_passed = contains_whole_word(actual_label, expected_label)\n",
    "        test_results.append(test_passed)\n",
    "\n",
    "        results = results._append({\n",
    "            'text': text,\n",
    "            'expected_label': expected_label,\n",
    "            'actual_label': actual_label,\n",
    "            'result': test_passed\n",
    "        }, ignore_index=True)\n",
    "        \n",
    "    end_time = time.time()\n",
    "    accuracy = sum(test_results) / len(test_results)\n",
    "    f1 = f1_score([True]*len(test_results), test_results, average='weighted')\n",
    "    precision = precision_score([True]*len(test_results), test_results, average='weighted')\n",
    "    recall = recall_score([True]*len(test_results), test_results, average='weighted')\n",
    "\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return accuracy, f1, precision, recall, time_taken, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1efa24d-7bbc-4197-bfcf-ba8801c71f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|███████████████████████████████████████████████▏                               | 505/846 [1:16:54<34:49,  6.13s/it]"
     ]
    }
   ],
   "source": [
    "for model in [\"mistral\", \"gemma\", \"openchat\"]:\n",
    "    accuracy, f1, precision, recall, time_taken, results = prompt(model, examples, cot=True, selfconsistency=False)\n",
    "\n",
    "    print('Accuracy cot {}: '.format(model), accuracy)\n",
    "    print('F1 score cot {}:'.format(model), f1)\n",
    "    print('Precision cot {}: '.format(model), precision)\n",
    "    print('Recall cot {}: '.format(model), recall)\n",
    "    \n",
    "    print(f'Time Taken: {time_taken:.2f} seconds\\n')\n",
    "    \n",
    "    results.to_csv(\"results/results-zero-shot-cot-{}.csv\".format(model), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c7012b-818d-4421-98fc-156d976d6b70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc6baf7-a4b5-4698-bb1c-5b0d1cc18e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
