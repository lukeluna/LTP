{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feeeb2ad-3443-408a-9948-6b40fdef61dd",
   "metadata": {},
   "source": [
    "## LTP Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d5200e1-fb0a-4bcd-b8eb-64a8206f8ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import ollama\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "add85130-8b6a-49f8-bfa0-94182bde3d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = 'data/handmade'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64098530-590d-4177-8d90-c0aa04741fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_hominem = pd.read_csv('{}/ad_hominem_final.csv'.format(DATA_FILEPATH))\n",
    "ad_populum = pd.read_csv('{}/ad_populum_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_anger = pd.read_csv('{}/appeal_to_anger_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_authority = pd.read_csv('{}/appeal_to_authority_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_fear = pd.read_csv('{}/appeal_to_fear_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_nature = pd.read_csv('{}/appeal_to_nature_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_pity = pd.read_csv('{}/appeal_to_pity_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_ridicule = pd.read_csv('{}/appeal_to_ridicule_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_tradition = pd.read_csv('{}/appeal_to_tradition_final.csv'.format(DATA_FILEPATH))\n",
    "appeal_to_worse_problems = pd.read_csv('{}/appeal_to_worse_problems_final.csv'.format(DATA_FILEPATH))\n",
    "causal_oversimplifiation = pd.read_csv('{}/causal_oversimplification_final.csv'.format(DATA_FILEPATH))\n",
    "equivocation = pd.read_csv('{}/equivocation_final.csv'.format(DATA_FILEPATH))\n",
    "fallacy_of_division = pd.read_csv('{}/fallacy_of_division_final.csv'.format(DATA_FILEPATH))\n",
    "false_analogy = pd.read_csv('{}/false_analogy_final.csv'.format(DATA_FILEPATH))\n",
    "false_causality = pd.read_csv('{}/false_causality_final.csv'.format(DATA_FILEPATH))\n",
    "false_dilemma = pd.read_csv('{}/false_dilemma_final.csv'.format(DATA_FILEPATH))\n",
    "hasty_generalization = pd.read_csv('{}/hasty_generalization_final.csv'.format(DATA_FILEPATH))\n",
    "nothing = pd.read_csv('{}/nothing_final.csv'.format(DATA_FILEPATH))\n",
    "slippery_slope = pd.read_csv('{}/slippery_slope_final.csv'.format(DATA_FILEPATH))\n",
    "strawman = pd.read_csv('{}/strawman_final.csv'.format(DATA_FILEPATH))\n",
    "circular_reasoning = pd.read_csv('{}/circular_reasoning.csv'.format(DATA_FILEPATH))\n",
    "tu_quoque = pd.read_csv('{}/tu_quoque.csv'.format(DATA_FILEPATH))\n",
    "\n",
    "mafalda = pd.read_csv('{}/MAFALDA.csv'.format('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd1b3dbf-68db-4416-8859-ed139fb29ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(is_mafalda=True):\n",
    "    list_of_tuples = []\n",
    "    nr_of_samples = 150\n",
    "    sample = False\n",
    "    \n",
    "    if is_mafalda:\n",
    "        datasets = [mafalda]\n",
    "    else:\n",
    "        datasets = [ad_hominem, ad_populum, appeal_to_anger, appeal_to_authority, appeal_to_fear, appeal_to_nature,\n",
    "                    appeal_to_pity, appeal_to_ridicule, appeal_to_tradition, appeal_to_worse_problems, causal_oversimplifiation,\n",
    "                    circular_reasoning, tu_quoque, #guilt_by_association,\n",
    "                    equivocation, fallacy_of_division, false_analogy, false_causality, false_dilemma,\n",
    "                    hasty_generalization, nothing, slippery_slope, strawman]\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        if sample:\n",
    "            if len(dataset) >= nr_of_samples:\n",
    "                # Sample 15 entries if the dataset is large enough\n",
    "                sampled = dataset.sample(n=nr_of_samples, random_state=random.randint(1, 100))  # Change seed for true randomness\n",
    "            else:\n",
    "                # If the dataset has less than 15 entries, take all available data\n",
    "                sampled = dataset\n",
    "        else:\n",
    "            sampled = dataset\n",
    "    \n",
    "        # Convert the sampled data to tuples and add to the list\n",
    "        list_of_tuples.extend(sampled.itertuples(index=False, name=None))\n",
    "\n",
    "    return list_of_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7306420-2a13-4225-99e4-04f16b87f441",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_prompt(model_id, message_content):\n",
    "    response = ollama.chat(model=model_id, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': message_content\n",
    "        }\n",
    "    ])\n",
    "    \n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "295ffa2e-7f01-4cb5-9a4d-6978ce402af5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_whole_word(large_string, word):\n",
    "    pattern = rf'\\b{re.escape(word)}\\b'\n",
    "    return bool(re.search(pattern, large_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f878606-43c2-4235-bba3-5221516b8d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rephrase_question(model_id, question):\n",
    "    rephrase_prompt = f\"Rephrase and expand the following question to improve clarity and detail: {question}\"\n",
    "    rephrase_response = ollama.chat(model_id, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': rephrase_prompt\n",
    "        }\n",
    "    ])\n",
    "    rephrased_question = rephrase_response['message']['content'].strip()\n",
    "    return rephrased_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "716f75d5-fe3e-45be-944c-08dd1eeb9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(model_id, rephrased_question):\n",
    "    response_prompt = rephrased_question\n",
    "    response = ollama.chat(model_id, messages=[\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': response_prompt\n",
    "        }\n",
    "    ])\n",
    "    return response['message']['content'].strip().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a37e472-3ad5-49c2-9126-11f7f251f0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt(model_id, message_base_content, is_mafalda=True, selfconsistency=False, rar=False):\n",
    "    test_results = []\n",
    "    results = pd.DataFrame()\n",
    "    start_time = time.time() \n",
    "\n",
    "    examples = prepare_dataset(is_mafalda=is_mafalda)\n",
    "    \n",
    "    for text, expected_label in tqdm(examples):\n",
    "        message_content = message_base_content.format(text)\n",
    "\n",
    "        if selfconsistency:\n",
    "            loop = 3\n",
    "            labels = [] \n",
    "            test_passeds = []\n",
    "            for i in range(loop):\n",
    "                subresponse = ollama_prompt(model_id, message_content)\n",
    "                label = subresponse.strip()\n",
    "                labels.append(label.partition('.')[0].lower())\n",
    "                test_passeds = contains_whole_word(label, expected_label)\n",
    "\n",
    "            c = Counter(test_passeds)\n",
    "            test_passed, _ = c.most_common()[0]\n",
    "            test_results.append(test_passed)\n",
    "    \n",
    "            results = results._append({\n",
    "                'text': text,\n",
    "                'expected_label': expected_label,\n",
    "                'actual_label1': labels[0],\n",
    "                'actual_label2': labels[1],\n",
    "                'actual_label3': labels[2],\n",
    "                'result': test_passed\n",
    "            }, ignore_index=True)\n",
    "        \n",
    "        else:\n",
    "            if rar:\n",
    "                rephrased_question = rephrase_question(model_id, message_content)\n",
    "                actual_label = get_response(model_id, rephrased_question)\n",
    "                test_passed = contains_whole_word(actual_label, expected_label)\n",
    "            else:\n",
    "                response = ollama_prompt(model_id, message_content)\n",
    "                actual_label = response.strip()\n",
    "                actual_label = actual_label.lower()\n",
    "            \n",
    "            test_passed = contains_whole_word(actual_label, expected_label)\n",
    "            test_results.append(test_passed)\n",
    "    \n",
    "            results = results._append({\n",
    "                'text': text,\n",
    "                'expected_label': expected_label,\n",
    "                'actual_label': actual_label,\n",
    "                'result': test_passed\n",
    "            }, ignore_index=True)\n",
    "            \n",
    "    end_time = time.time()\n",
    "    accuracy = sum(test_results) / len(test_results)\n",
    "    f1 = f1_score([True]*len(test_results), test_results, average='weighted')\n",
    "    precision = precision_score([True]*len(test_results), test_results, average='weighted')\n",
    "    recall = recall_score([True]*len(test_results), test_results, average='weighted')\n",
    "\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    return accuracy, f1, precision, recall, time_taken, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f1efa24d-7bbc-4197-bfcf-ba8801c71f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_prompt(message_base_content, title=\"Tot\", is_mafalda=True, selfconsistency=False, rar=False):\n",
    "    for model in [\"mistral\", \"gemma\", \"openchat\"]:\n",
    "        accuracy, f1, precision, recall, time_taken, results = prompt(model, message_base_content, is_mafalda, selfconsistency, rar)\n",
    "    \n",
    "        print('Accuracy {} {}: '.format(title, model), accuracy)\n",
    "        print('F1 score {} {}:'.format(title, model), f1)\n",
    "        print('Precision {} {}: '.format(title, model), precision)\n",
    "        print('Recall {} {}: '.format(title, model), recall)\n",
    "        \n",
    "        print(f'Time Taken: {time_taken:.2f} seconds\\n')\n",
    "        \n",
    "        results.to_csv(\"results/results-{}-{}.csv\".format(title, model), sep=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3d606b8b-9ea2-4e24-9520-3880f2dc796e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath_zs_tot = 'ToT_prompts/zero_shot_tot.txt'\n",
    "filepath_tot = 'ToT_prompts/tot_with_examples.txt'\n",
    "filepath_rar_1 = 'RaR/RaR_old.txt'\n",
    "filepath_rar_2 = 'RaR/RaR_new.txt'\n",
    "filepath_cot = 'CoT_prompts/few_shot_cot_update.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6c8333e-f76b-4129-9865-959a913d3c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(filepath_zs_tot, 'r') as file:\n",
    "    message_base_content_zs_tot = file.read()\n",
    "file.close()\n",
    "\n",
    "with open(filepath_tot, 'r') as file:\n",
    "    message_base_content_tot = file.read()\n",
    "file.close()\n",
    "\n",
    "with open(filepath_cot, 'r') as file:\n",
    "    message_base_content_cot = file.read()\n",
    "file.close()\n",
    "\n",
    "with open(filepath_rar_1, 'r') as file:\n",
    "    message_base_content_rar_1 = file.read()\n",
    "file.close()\n",
    "\n",
    "with open(filepath_rar_2, 'r') as file:\n",
    "    message_base_content_rar_2 = file.read()\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5615d56c-277a-4ac1-b8dd-61235bf3c976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_prompt(message_base_content_cot, title=\"cot-sc\", is_mafalda=True, selfconsistency=True, rar=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
